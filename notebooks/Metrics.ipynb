{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da8dcca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu, corpus_bleu\n",
    "from nsd_access import NSDAccess\n",
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4dcc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = f'/home/hpcgies1/Masters-Thesis/AttemptFour/'\n",
    "eval_dir = f'{home_dir}/Log/'\n",
    "\n",
    "# model = 'all_subjects'\n",
    "# epoch = 71\n",
    "# model = 'subject_2_baseline2'\n",
    "# epoch = 80\n",
    "# model = 'subject_2_both_layer_norm'\n",
    "# epoch = 25\n",
    "# model = 'subject_2_lstm_layer_norm'\n",
    "# epoch = 44\n",
    "# model = 'subject_2_dot_product'\n",
    "# epoch = '46'\n",
    "\n",
    "models = ['subject_1_baseline', 'subject_2_baseline2', 'subject_5_baseline', 'subject_7_baseline']\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "model_path   = glob.glob(f\"{eval_dir}/{model}/eval_out/output_captions_*\")[0]\n",
    "# model_path = f'/home/hpcgies1/Masters-Thesis/AttemptFour/Log/{model}/eval_out/output_captions_{epoch}.npy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d78e2",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dea1b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd_loader = NSDAccess('/home/hpcgies1/rds/hpc-work/NIC/NSD/')\n",
    "nsd_loader.stim_descriptions = pd.read_csv(nsd_loader.stimuli_description_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37377806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    return np.squeeze(np.load(open(fname, 'rb')), axis=-1)\n",
    "\n",
    "def load_tokenizer(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        tok =tokenizer_from_json(f.read())\n",
    "    return tok\n",
    "\n",
    "def remove_pad_end(cap: str):\n",
    "    cap = cap.split(\" \")\n",
    "    cap = [i for i in cap if i != '<pad>' and i != '<end>']\n",
    "    return \" \".join(cap)\n",
    "\n",
    "def get_target_caption(key):\n",
    "    \"\"\" Return target caption for a given key in [1,73000] \"\"\"\n",
    "    with HiddenPrints():\n",
    "        target = nsd_loader.read_image_coco_info([int(key)-1]) # returns list(dict)\n",
    "        target = target[0]['caption'] # get first target caption\n",
    "    return target\n",
    "\n",
    "def get_target_captions(keys: list):\n",
    "    \"\"\" Return target caption for a given key in [1,73000] \"\"\"\n",
    "    keys = [int(i)-1 for i in keys]\n",
    "    output_targets = []\n",
    "    with HiddenPrints():\n",
    "        targets = nsd_loader.read_image_coco_info(keys) # returns list(list(dict))\n",
    "    for _, t in enumerate(targets):\n",
    "        ts = []\n",
    "        for i in range(5):\n",
    "            target = t[i]['caption'] # get target captions\n",
    "            ts.append(target)\n",
    "        output_targets.append(ts)\n",
    "    return output_targets\n",
    "\n",
    "def clean_targets(targets: list):\n",
    "    \"\"\" given list of list of targets: return cleaned strings \"\"\"\n",
    "    new = []\n",
    "    for i in range(len(targets)):\n",
    "        ts = []\n",
    "        for k in range(5):\n",
    "            t = targets[i][k]\n",
    "            t = t.replace(\".\",\" \").replace(\",\", \" \").strip().split(\" \")\n",
    "            t = [n.lower() for n in t if n != '']\n",
    "            t = \" \".join(t)\n",
    "            ts.append(t)\n",
    "        new.append(ts)\n",
    "    return new\n",
    "\n",
    "class HiddenPrints:\n",
    "    \"\"\" Use with with HiddenPrints() to temporarily surpress print output \"\"\"\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ecfc5f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b87d46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(515, 15)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(f'/home/hpcgies1/Masters-Thesis/AttemptFour/Log/{model}/eval_out/tokenizer.json')\n",
    "test_keys = pd.read_csv(f'{home_dir}/TrainData/subj02_conditions2.csv')\n",
    "test_keys = test_keys['nsd_key'].loc[test_keys['is_test'] == 1].values\n",
    "output = load_data(model_path)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c381c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "captions = tokenizer.sequences_to_texts(output)\n",
    "print(len(captions))\n",
    "targets = get_target_captions(test_keys)\n",
    "targets = clean_targets(targets)\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "011eff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45252029450632436\n",
      "0.23828125008342665\n",
      "0.12126085657826138\n",
      "0.06475528175925194\n"
     ]
    }
   ],
   "source": [
    "def compute_bleu(captions: list, targets: list):\n",
    "    captions = [remove_pad_end(c) for c in captions]\n",
    "    \n",
    "    weights = [\n",
    "        (1, 0, 0, 0),\n",
    "        (0, 1, 0, 0),\n",
    "        (0, 0, 1, 0),\n",
    "        (0, 0, 0, 1),\n",
    "        (1./1., 0, 0, 0),\n",
    "        (1./2., 1./2., 0, 0),\n",
    "        (1./3., 1./3., 1./3., 0),\n",
    "        (1./4., 1./4., 1./4., 1./4.)\n",
    "    ]\n",
    "    \n",
    "    hypothesis = []\n",
    "    references = []\n",
    "    for i in range(1):\n",
    "        caps = captions[i*515:i*515+515]\n",
    "        for i in range(len(caps)):\n",
    "            ref = [i.split(\" \") for i in targets[i]]\n",
    "            hyp = caps[i].split(\" \")\n",
    "            hypothesis.append(hyp)\n",
    "            references.append(ref)\n",
    "\n",
    "    chencherry = SmoothingFunction()\n",
    "    for w in weights[4:]:\n",
    "        b_score = corpus_bleu(references, hypothesis, weights=w, smoothing_function=chencherry.method0)\n",
    "        print(b_score)\n",
    "    return b_score\n",
    "\n",
    "bleu = compute_bleu(captions, targets)\n",
    "# print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fd7ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub: 1 - 0.154\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Fraction(0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m b_score\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSub: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(compute_bleu_single(captions[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m515\u001b[39m:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m515\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m515\u001b[39m], targets))\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mcompute_bleu_single\u001b[0;34m(captions, targets)\u001b[0m\n\u001b[1;32m     22\u001b[0m     hypothesis\u001b[38;5;241m.\u001b[39mappend(hyp)\n\u001b[1;32m     23\u001b[0m     references\u001b[38;5;241m.\u001b[39mappend(ref)\n\u001b[0;32m---> 25\u001b[0m b_score \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchencherry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m b_score\n",
      "File \u001b[0;32m~/.conda/envs/pycor2/lib/python3.8/site-packages/nltk/translate/bleu_score.py:224\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    221\u001b[0m bp \u001b[38;5;241m=\u001b[39m brevity_penalty(ref_lengths, hyp_lengths)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Collects the various precision values for the different ngram orders.\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m p_n \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    225\u001b[0m     Fraction(p_numerators[i], p_denominators[i], _normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    227\u001b[0m ]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Returns 0 if there's no matching n-grams\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# We only need to check for p_numerators[1] == 0, since if there's\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# no unigrams, there won't be any higher order ngrams.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p_numerators[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/pycor2/lib/python3.8/site-packages/nltk/translate/bleu_score.py:225\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    221\u001b[0m bp \u001b[38;5;241m=\u001b[39m brevity_penalty(ref_lengths, hyp_lengths)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Collects the various precision values for the different ngram orders.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m p_n \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mFraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_numerators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_denominators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    227\u001b[0m ]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Returns 0 if there's no matching n-grams\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# We only need to check for p_numerators[1] == 0, since if there's\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# no unigrams, there won't be any higher order ngrams.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p_numerators[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/pycor2/lib/python3.8/fractions.py:178\u001b[0m, in \u001b[0;36mFraction.__new__\u001b[0;34m(cls, numerator, denominator, _normalize)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth arguments should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRational instances\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denominator \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFraction(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, 0)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m numerator)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _normalize:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(numerator) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(denominator):\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# *very* normal case\u001b[39;00m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Fraction(0, 0)"
     ]
    }
   ],
   "source": [
    "def compute_bleu_single(captions: list, targets: list):\n",
    "    captions = [remove_pad_end(c) for c in captions]\n",
    "    \n",
    "    chencherry = SmoothingFunction()\n",
    "    weights = [\n",
    "        (1, 0, 0, 0),\n",
    "        (0, 1, 0, 0),\n",
    "        (0, 0, 1, 0),\n",
    "        (0, 0, 0, 1),\n",
    "        (1./1., 0, 0, 0),\n",
    "        (1./2., 1./2., 0, 0),\n",
    "        (1./3., 1./3., 1./3., 0),\n",
    "        (1./4., 1./4., 1./4., 1./4.)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    hypothesis = []\n",
    "    references = []\n",
    "    for i in range(len(captions)):\n",
    "        ref = [i.split(\" \") for i in targets[i]]\n",
    "        hyp = captions[i].split(\" \")\n",
    "        hypothesis.append(hyp)\n",
    "        references.append(ref)\n",
    "    \n",
    "    b_score = corpus_bleu(references, hypothesis, weights=weights[-1], smoothing_function=chencherry.method0)\n",
    "    return b_score\n",
    "\n",
    "for i in range(8):\n",
    "    print(f\"Sub: {i+1} - {(compute_bleu_single(captions[i*515:i*515+515], targets)):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe54ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d00cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
