{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8dcca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 11:23:03.451355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/daal/lib/intel64_lin:/usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/ipp/lib/intel64:/usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/tbb/lib/intel64/gcc4.7:/usr/local/Cluster-Apps/intel/2017.4/debugger_2017/iga/lib:/usr/local/Cluster-Apps/intel/2017.4/debugger_2017/libipt/intel64/lib:/usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/mpi/intel64/lib:/usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/tbb/lib/intel64_lin/gcc4.7:/usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/mkl/lib/intel64_lin:/usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64:/usr/local/Cluster-Apps/intel/2017.4/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64_lin:/usr/local/software/global/lib:/usr/local/Cluster-Apps/vgl/2.5.1/64/lib:/usr/local/software/slurm/current/lib\n",
      "2022-06-08 11:23:03.451416: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu, corpus_bleu\n",
    "from nsd_access import NSDAccess\n",
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4dcc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = f'/home/hpcgies1/Masters-Thesis/AttemptFour/'\n",
    "eval_dir = f'{home_dir}/Log/'\n",
    "\n",
    "# model = 'all_subjects'\n",
    "# epoch = 71\n",
    "# model = 'subject_2_baseline2'\n",
    "# epoch = 80\n",
    "# model = 'subject_2_both_layer_norm'\n",
    "# epoch = 25\n",
    "# model = 'subject_2_lstm_layer_norm'\n",
    "# epoch = 44\n",
    "# model = 'subject_2_dot_product'\n",
    "# epoch = '46'\n",
    "\n",
    "models = ['subject_1_baseline', 'subject_2_baseline2', 'subject_5_baseline', 'subject_7_baseline']\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "model_path   = glob.glob(f\"{eval_dir}/{model}/eval_out/output_captions_*\")[0]\n",
    "# model_path = f'/home/hpcgies1/Masters-Thesis/AttemptFour/Log/{model}/eval_out/output_captions_{epoch}.npy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d78e2",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea1b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd_loader = NSDAccess('/home/hpcgies1/rds/hpc-work/NIC/NSD/')\n",
    "nsd_loader.stim_descriptions = pd.read_csv(nsd_loader.stimuli_description_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37377806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    return np.squeeze(np.load(open(fname, 'rb')), axis=-1)\n",
    "\n",
    "def load_tokenizer(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        tok =tokenizer_from_json(f.read())\n",
    "    return tok\n",
    "\n",
    "def remove_pad_end(cap: str):\n",
    "    cap = cap.split(\" \")\n",
    "    cap = [i for i in cap if i != '<pad>' and i != '<end>']\n",
    "    return \" \".join(cap)\n",
    "\n",
    "def get_target_caption(key):\n",
    "    \"\"\" Return target caption for a given key in [1,73000] \"\"\"\n",
    "    with HiddenPrints():\n",
    "        target = nsd_loader.read_image_coco_info([int(key)-1]) # returns list(dict)\n",
    "        target = target[0]['caption'] # get first target caption\n",
    "    return target\n",
    "\n",
    "def get_target_captions(keys: list):\n",
    "    \"\"\" Return target caption for a given key in [1,73000] \"\"\"\n",
    "    keys = [int(i)-1 for i in keys]\n",
    "    output_targets = []\n",
    "    with HiddenPrints():\n",
    "        targets = nsd_loader.read_image_coco_info(keys) # returns list(list(dict))\n",
    "    for _, t in enumerate(targets):\n",
    "        ts = []\n",
    "        for i in range(5):\n",
    "            target = t[i]['caption'] # get target captions\n",
    "            ts.append(target)\n",
    "        output_targets.append(ts)\n",
    "    return output_targets\n",
    "\n",
    "def clean_targets(targets: list):\n",
    "    \"\"\" given list of list of targets: return cleaned strings \"\"\"\n",
    "    new = []\n",
    "    for i in range(len(targets)):\n",
    "        ts = []\n",
    "        for k in range(5):\n",
    "            t = targets[i][k]\n",
    "            t = t.replace(\".\",\" \").replace(\",\", \" \").strip().split(\" \")\n",
    "            t = [n.lower() for n in t if n != '']\n",
    "            t = \" \".join(t)\n",
    "            ts.append(t)\n",
    "        new.append(ts)\n",
    "    return new\n",
    "\n",
    "class HiddenPrints:\n",
    "    \"\"\" Use with with HiddenPrints() to temporarily surpress print output \"\"\"\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ecfc5f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b87d46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(515, 15)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(f'/home/hpcgies1/Masters-Thesis/AttemptFour/Log/{model}/eval_out/tokenizer.json')\n",
    "test_keys = pd.read_csv(f'{home_dir}/TrainData/subj02_conditions2.csv')\n",
    "test_keys = test_keys['nsd_key'].loc[test_keys['is_test'] == 1].values\n",
    "output = load_data(model_path)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f8d8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(515, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(f'/home/hpcgies1/rds/hpc-work/NIC/Log/torch_alt_batches/eval_out/tokenizer.json')\n",
    "test_keys = pd.read_csv(f'{home_dir}/TrainData/subj02_conditions2.csv')\n",
    "test_keys = test_keys['nsd_key'].loc[test_keys['is_test'] == 1].values\n",
    "output = np.load('/home/hpcgies1/rds/hpc-work/NIC/Log/torch_alt_batches/eval_out/output_captions_8.npy')\n",
    "output = output[1]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c381c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "captions = tokenizer.sequences_to_texts(output)\n",
    "print(len(captions))\n",
    "targets = get_target_captions(test_keys)\n",
    "targets = clean_targets(targets)\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "011eff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4985661614092585\n",
      "0.2926175940448853\n",
      "0.1740020290350698\n",
      "0.11607832773724937\n"
     ]
    }
   ],
   "source": [
    "def compute_bleu(captions: list, targets: list):\n",
    "    captions = [remove_pad_end(c) for c in captions]\n",
    "    \n",
    "    weights = [\n",
    "        (1, 0, 0, 0),\n",
    "        (0, 1, 0, 0),\n",
    "        (0, 0, 1, 0),\n",
    "        (0, 0, 0, 1),\n",
    "        (1./1., 0, 0, 0),\n",
    "        (1./2., 1./2., 0, 0),\n",
    "        (1./3., 1./3., 1./3., 0),\n",
    "        (1./4., 1./4., 1./4., 1./4.)\n",
    "    ]\n",
    "    \n",
    "    hypothesis = []\n",
    "    references = []\n",
    "    for i in range(1):\n",
    "        caps = captions[i*515:i*515+515]\n",
    "        for i in range(len(caps)):\n",
    "            ref = [i.split(\" \") for i in targets[i]]\n",
    "            hyp = caps[i].split(\" \")\n",
    "            hypothesis.append(hyp)\n",
    "            references.append(ref)\n",
    "\n",
    "    chencherry = SmoothingFunction()\n",
    "    for w in weights[4:]:\n",
    "        b_score = corpus_bleu(references, hypothesis, weights=w, smoothing_function=chencherry.method0)\n",
    "        print(b_score)\n",
    "    return b_score\n",
    "\n",
    "bleu = compute_bleu(captions, targets)\n",
    "# print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f8a6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_(candidate, targets):\n",
    "    \"\"\" Compute BLEU between a candidate caption and its targets\n",
    "    Parameters:\n",
    "    -----------\n",
    "        candidate : str\n",
    "        targets : list of strings\n",
    "    \"\"\"\n",
    "    candidate = candidate.split(\" \")\n",
    "    candidate = [remove_pad_end(c) for c in candidate]\n",
    "    \n",
    "    weights = [\n",
    "        (1, 0, 0, 0),  # Bleu-1\n",
    "        (0, 0, 0, 1),  # Bleu-4\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    hypothesis = []\n",
    "    references = []\n",
    "    for i in range(5):\n",
    "        ref = targets[i].split(\" \")\n",
    "        references.append(ref)\n",
    "\n",
    "    \n",
    "    chencherry = SmoothingFunction()\n",
    "    b_scores = []\n",
    "    for w in weights:\n",
    "        b_score = sentence_bleu(references, candidate, weights=w, smoothing_function=chencherry.method1)\n",
    "        b_scores.append(b_score)\n",
    "    return b_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3fe54ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333 0.008333333333333335\n",
      "\ta man riding a skateboard down a street <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\ta person in a wetsuit surfing on a turquoise wave\n",
      "\n",
      "0.4 0.08333333333333333\n",
      "\ta man riding a wave on top of a surfboard <end> <pad> <pad> <pad> <pad>\n",
      "\ta windsurfer some water a hill sand and some kayaks\n",
      "\n",
      "0.5333333333333333 0.008333333333333335\n",
      "\ta plate of food with a fork and a fork <end> <pad> <pad> <pad> <pad>\n",
      "\ta plate of yummy food of some kind\n",
      "\n",
      "0.5333333333333333 0.08333333333333333\n",
      "\ta plate with a sandwich and a fork on it <end> <pad> <pad> <pad> <pad>\n",
      "\ta cup of coffee on a plate with a spoon\n",
      "\n",
      "0.26666666666666666 0.008333333333333335\n",
      "\ta man is standing on a tennis court holding a racquet <end> <pad> <pad> <pad>\n",
      "\tthree young women are trying to catch a frisbee\n",
      "\n",
      "0.4666666666666667 0.008333333333333335\n",
      "\ta large airplane flying through the sky <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\ta lone airplane is flying high against a grey sky\n",
      "\n",
      "0.26666666666666666 0.008333333333333335\n",
      "\ta man is playing tennis on a tennis court <end> <pad> <pad> <pad> <pad> <pad>\n",
      "\ta young child riding a wave on top of a board\n",
      "\n",
      "0.3333333333333333 0.008333333333333335\n",
      "\ta man in a suit and tie standing in front of a building <end> <pad>\n",
      "\ta bear that is standing in the water\n",
      "\n",
      "0.3118356616772059 0.007795891541930149\n",
      "\ta man and a woman standing in front of a cake <end> <pad> <pad> <pad>\n",
      "\tman laying on top of bed in green towel working on laptop\n",
      "\n",
      "0.2 0.008333333333333335\n",
      "\ta man riding a skateboard down a street <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\ta train traveling over a river on a bridge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(*bleu_(captions[i], targets[i]))\n",
    "    print(f\"\\t{captions[i]}\")\n",
    "    print(f\"\\t{targets[i][0]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d00cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
