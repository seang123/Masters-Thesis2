{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8dcca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 13:54:13.022645: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/software/spack/spack-0.11.2/opt/spack/linux-rhel7-x86_64/gcc-5.4.0/openmpi-1.10.7-jdc7f4fjdq5roxhadufd6h66xkwuytss/lib:/usr/local/software/spack/spack-0.11.2/opt/spack/linux-rhel7-x86_64/gcc-4.8.5/gcc-5.4.0-fis24ggupugiobii56fesif2y3qulpdr/lib64:/usr/local/software/spack/spack-0.11.2/opt/spack/linux-rhel7-x86_64/gcc-4.8.5/gcc-5.4.0-fis24ggupugiobii56fesif2y3qulpdr/lib:/usr/local/Cluster-Apps/cuda/8.0/lib64:/usr/local/Cluster-Apps/cuda/8.0/lib:/usr/local/software/global/lib:/usr/local/Cluster-Apps/vgl/2.5.1/64/lib:/usr/local/software/slurm/current/lib\n",
      "2022-04-21 13:54:13.022713: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu, corpus_bleu\n",
    "from nsd_access import NSDAccess\n",
    "import os, sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4dcc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'all_subjects'\n",
    "epoch = 71\n",
    "# model = 'subject_2_baseline2'\n",
    "# epoch = 80\n",
    "# model = 'subject_2_both_layer_norm'\n",
    "# epoch = 25\n",
    "\n",
    "model_path = f'/home/hpcgies1/Masters-Thesis/AttemptFour/Log/{model}/eval_out/output_captions_{epoch}.npy'\n",
    "home_dir = f'/home/hpcgies1/Masters-Thesis/AttemptFour/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d78e2",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea1b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd_loader = NSDAccess('/home/hpcgies1/rds/hpc-work/NIC/NSD/')\n",
    "nsd_loader.stim_descriptions = pd.read_csv(nsd_loader.stimuli_description_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37377806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    return np.squeeze(np.load(open(fname, 'rb')), axis=-1)\n",
    "\n",
    "def load_tokenizer(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        tok =tokenizer_from_json(f.read())\n",
    "    return tok\n",
    "\n",
    "def remove_pad_end(cap: str):\n",
    "    cap = cap.split(\" \")\n",
    "    cap = [i for i in cap if i != '<pad>' and i != '<end>']\n",
    "    return \" \".join(cap)\n",
    "\n",
    "def get_target_caption(key):\n",
    "    \"\"\" Return target caption for a given key in [1,73000] \"\"\"\n",
    "    with HiddenPrints():\n",
    "        target = nsd_loader.read_image_coco_info([int(key)-1]) # returns list(dict)\n",
    "        target = target[0]['caption'] # get first target caption\n",
    "    return target\n",
    "\n",
    "def get_target_captions(keys: list):\n",
    "    \"\"\" Return target caption for a given key in [1,73000] \"\"\"\n",
    "    keys = [int(i)-1 for i in keys]\n",
    "    output_targets = []\n",
    "    with HiddenPrints():\n",
    "        targets = nsd_loader.read_image_coco_info(keys) # returns list(list(dict))\n",
    "    for _, t in enumerate(targets):\n",
    "        ts = []\n",
    "        for i in range(5):\n",
    "            target = t[i]['caption'] # get target captions\n",
    "            ts.append(target)\n",
    "        output_targets.append(ts)\n",
    "    return output_targets\n",
    "\n",
    "def clean_targets(targets: list):\n",
    "    \"\"\" given list of list of targets: return cleaned strings \"\"\"\n",
    "    new = []\n",
    "    for i in range(len(targets)):\n",
    "        ts = []\n",
    "        for k in range(5):\n",
    "            t = targets[i][k]\n",
    "            t = t.replace(\".\",\" \").replace(\",\", \" \").strip().split(\" \")\n",
    "            t = [n.lower() for n in t if n != '']\n",
    "            t = \" \".join(t)\n",
    "            ts.append(t)\n",
    "        new.append(ts)\n",
    "    return new\n",
    "\n",
    "class HiddenPrints:\n",
    "    \"\"\" Use with with HiddenPrints() to temporarily surpress print output \"\"\"\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ecfc5f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b87d46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4120, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(f'/home/hpcgies1/Masters-Thesis/AttemptFour/Log/{model}/eval_out/tokenizer.json')\n",
    "test_keys = pd.read_csv(f'{home_dir}/TrainData/subj02_conditions2.csv')\n",
    "test_keys = test_keys['nsd_key'].loc[test_keys['is_test'] == 1].values\n",
    "output = load_data(model_path)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c381c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4120\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "captions = tokenizer.sequences_to_texts(output)\n",
    "print(len(captions))\n",
    "targets = get_target_captions(test_keys)\n",
    "targets = clean_targets(targets)\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011eff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5601214574898785\n",
      "0.374840023975205\n",
      "0.2513694386607573\n",
      "0.17357007227610402\n",
      "0.17357007227610402\n"
     ]
    }
   ],
   "source": [
    "def compute_bleu(captions: list, targets: list):\n",
    "    captions = [remove_pad_end(c) for c in captions]\n",
    "    \n",
    "    weights = [\n",
    "        (1, 0, 0, 0),\n",
    "        (0, 1, 0, 0),\n",
    "        (0, 0, 1, 0),\n",
    "        (0, 0, 0, 1),\n",
    "        (1./1., 0, 0, 0),\n",
    "        (1./2., 1./2., 0, 0),\n",
    "        (1./3., 1./3., 1./3., 0),\n",
    "        (1./4., 1./4., 1./4., 1./4.)\n",
    "    ]\n",
    "    \n",
    "    hypothesis = []\n",
    "    references = []\n",
    "    for i in range(1):\n",
    "        caps = captions[i*515:i*515+515]\n",
    "        for i in range(len(caps)):\n",
    "            ref = [i.split(\" \") for i in targets[i]]\n",
    "            hyp = caps[i].split(\" \")\n",
    "            hypothesis.append(hyp)\n",
    "            references.append(ref)\n",
    "\n",
    "    chencherry = SmoothingFunction()\n",
    "    for w in weights[4:]:\n",
    "        b_score = corpus_bleu(references, hypothesis, weights=w, smoothing_function=chencherry.method0)\n",
    "        print(b_score)\n",
    "    return b_score\n",
    "\n",
    "bleu = compute_bleu(captions, targets)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fd7ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub: 1 - 0.174\n",
      "Sub: 2 - 0.172\n",
      "Sub: 3 - 0.161\n",
      "Sub: 4 - 0.147\n",
      "Sub: 5 - 0.181\n",
      "Sub: 6 - 0.163\n",
      "Sub: 7 - 0.170\n",
      "Sub: 8 - 0.121\n"
     ]
    }
   ],
   "source": [
    "def compute_bleu_single(captions: list, targets: list):\n",
    "    captions = [remove_pad_end(c) for c in captions]\n",
    "    \n",
    "    chencherry = SmoothingFunction()\n",
    "    weights = [\n",
    "        (1, 0, 0, 0),\n",
    "        (0, 1, 0, 0),\n",
    "        (0, 0, 1, 0),\n",
    "        (0, 0, 0, 1),\n",
    "        (1./1., 0, 0, 0),\n",
    "        (1./2., 1./2., 0, 0),\n",
    "        (1./3., 1./3., 1./3., 0),\n",
    "        (1./4., 1./4., 1./4., 1./4.)\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    hypothesis = []\n",
    "    references = []\n",
    "    for i in range(len(captions)):\n",
    "        ref = [i.split(\" \") for i in targets[i]]\n",
    "        hyp = captions[i].split(\" \")\n",
    "        hypothesis.append(hyp)\n",
    "        references.append(ref)\n",
    "    \n",
    "    b_score = corpus_bleu(references, hypothesis, weights=weights[-1], smoothing_function=chencherry.method0)\n",
    "    return b_score\n",
    "\n",
    "for i in range(8):\n",
    "    print(f\"Sub: {i+1} - {(compute_bleu_single(captions[i*515:i*515+515], targets)):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe54ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d00cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
