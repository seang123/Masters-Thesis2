---
 # Name the current run
 run: torch_test

 info: | 
     testing that the pytorch imp is actually correct for 1 sub
     subs 2
     layer norm on encoder and lstm hidden state
     pytorch model
     8 subjects
     layer norm on lstm and encoders

 log: "/home/hpcgies1/rds/hpc-work/NIC/Log/"

 # Data stores
 dataset:
     #betas_path: "/fast/seagie/data/subj_2/betas_averaged/"
     betas_path: "/home/hpcgies1/rds/hpc-work/NIC/Data/subj_2/betas_averaged/"
     #captions_path: "/fast/seagie/data/subj_2/captions/"
     captions_path: "/home/hpcgies1/rds/hpc-work/NIC/Data/captions/"
     nsd_dir: "/home/hpcgies1/rds/hpc-work/NIC/NSD"

 seed: 42

 # Training
 epochs: 50
 batch_size: 128
 max_length: 15
 top_k: 5000 # vocab size
 optimizer: Adam
 alpha: 0.0001 # 0.0001
 clipnorm: 0.1
 decay: 0 #1.0e-4

 dropout_input: 0
 dropout_features: 0.1
 dropout_text: 0.1
 dropout_lstm: 0.1
 dropout_attn: 0.1 
 dropout_out: 0.1

 input_reg: 0.01         # scientific notation requires decimal notation - x.0e
 attn_reg: 0.001
 lstm_reg: 0.00003
 output_reg: 0.00001 

 # Input size
 input: 
     full: 327684
     vc: 62756
     pca: 5000
     mscoco: 4096

 # Model size 
 units: 512 # 512 # 2048 # lstm
 attn_units: 32
 group_size: 32 # 32 # acts as embedding dim for attention model
 embedding_features: 512
 embedding_text: 512





